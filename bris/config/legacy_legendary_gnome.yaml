defaults:
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none
  - _self_
  
start_date: 2024-11-24T12:00:00
end_date: 2024-11-24T18:00:00

checkpoint_path: /lustre/storeB/project/nwp/bris/aram/fix-memory-issue/experiments2/inference-aifs-by_step-epoch_000-step_000150.ckpt

leadtimes: 12
timestep: 6h
frequency: 6h

deterministic: True

lam_dataset: ${hardware.paths.data}${hardware.files.lam_dataset}
global_dataset: ${hardware.paths.data}${hardware.files.global_dataset}

# If the user wants to release GPU cache and memory
# This option releases unused cached/memory used by torch
release_cache: False

dataloader:
  batch_size: 1
  prefetch_factor: 2
  num_workers: 1
  pin_memory: True

  read_group_size: 1 #Do not change this, not implemented properly

  predict:
    cutout:
      - dataset: ${lam_dataset}
      - dataset: ${global_dataset}
        rename: 
          q_600 : tp  # tp does not exist in the global dataset. tp is diagnostic, rename is sufficient
    min_distance_km: 0
    adjust: all 

  datamodule:
    _target_: anemoi.training.data.dataset.NativeGridDataset
    _convert_: all

hardware:
  paths:
    data: /lustre/storeB/project/nwp/bris/aram/fix-memory-issue/debug_files/
  files:
    lam_dataset: meps_20241124T18Z.zarr 
    global_dataset: ifs_20241124T18Z.zarr
    dataset_obs: name_of_dataset

  num_gpus_per_node: 1
  num_gpus_per_model: 1
  num_nodes: 1

model:
  _target_: bris.model.BrisPredictor
  _convert_: all


checkpoints:
  - my_interpolator:
    type: interpolator
    path: interpolator.ckpt
  - my_forecaster:
    type: forecaster
    path: forecast.ckpt

routing:
  - decoder_index: 0
    domain: 0
    outputs:
      - netcdf:
          filename_pattern: meps_pred_%Y%m%dT%HZ.nc
          variables: [2t, 2d]
  - decoder_index: 0
    domain: 1
    outputs:
      - netcdf:
          filename_pattern: era_pred_%Y%m%dT%HZ.nc
          variables: [2t, 2d]

